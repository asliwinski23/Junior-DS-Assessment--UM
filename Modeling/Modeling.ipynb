{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4767ef9",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9224517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stats\n",
    "from sklearn import preprocessing\n",
    "import plotly.express as px\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.base import clone\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289282e0",
   "metadata": {},
   "source": [
    "## 2. Import cleaned train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb98693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31f49d",
   "metadata": {},
   "source": [
    "## 3. Creating Functions for Modeling\n",
    "### 3a) Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae0111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split function with scaling\n",
    "# test train split function\n",
    "def set_train_test(data, input_seed, pct_test):\n",
    "    \"\"\"Returns train_df (pd.DataFrame), test_df (pd.DataFrame) which are randomly split training and testing data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    data (pd.DataFrame) -- All the data, must include all features that you want to look at and also target.\n",
    "        Optionally, can include other random columns.\n",
    "    input_seed (int) -- seed for randomization.\n",
    "    pct_test (float) -- must be between 0 and 1 inclusive. The percent of data used for testing.\n",
    "    \"\"\"\n",
    "    train_df, test_df = train_test_split(\n",
    "        data,\n",
    "        test_size = pct_test, \n",
    "        random_state = input_seed\n",
    "    )\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bc7b6",
   "metadata": {},
   "source": [
    "### 3b) Outline model stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50143dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_acc_stats(y_actual_test, y_pred_test, model_name, features):\n",
    "    \"\"\"Returns model_stats (pd.DataFrame) which contains performance data for the model.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y_actual (pd.Series or numpy.ndarray) -- the actual y values.\n",
    "    y_pred (pd.Series or numpy.ndarray) -- the predicted y values.\n",
    "    model_name (str) -- what you want to call the model. it will be saved in a csv later.\n",
    "    features (list) -- list of the column names used in this model to create the predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # test metrics\n",
    "    test_accuracy = accuracy_score(y_actual_test, y_pred_test) # real y goes first, predicted y goes second\n",
    "    test_precision = precision_score(y_actual_test, y_pred_test)\n",
    "    test_recall = recall_score(y_actual_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_actual_test, y_pred_test)\n",
    "    test_roc_auc = roc_auc_score(y_actual_test, y_pred_test)\n",
    "\n",
    "    # make a dataframe with model results used to save results into a CSV\n",
    "    model_stats = pd.DataFrame( \n",
    "            {\n",
    "                'model_name':model_name, # this is the model type I testing\n",
    "                'features':[features], \n",
    "                'test_accuracy':test_accuracy,\n",
    "                'test_precision':test_precision,\n",
    "                'test_recall':test_recall,\n",
    "                'test_f1':test_f1,\n",
    "                'test_roc_auc':test_roc_auc\n",
    "            }\n",
    "    )\n",
    "    return model_stats\n",
    "\n",
    "# write stats to a csv\n",
    "def helper_write(stats, file_path):\n",
    "    try: # try means it will \"try\" the following code. if it results in an error, then it will stop and jump to except\n",
    "        df_results = pd.read_csv(file_path) # this returns an error if results.csv isn't an actual file. \n",
    "        # if there is df_results, then add new results to it.\n",
    "\n",
    "        df_results = pd.concat([df_results, stats])\n",
    "        df_results.to_csv(file_path, index=False)\n",
    "    except:\n",
    "        print(f'There is no CSV called {file_path}')\n",
    "        stats.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc9a33",
   "metadata": {},
   "source": [
    "### 3c) Outline how to evaluate model, for forward step feature selection, testing, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0dcb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(input_name, input_model, training_data, testing_data, features, target):\n",
    "    \"\"\"Writes test results of a model using training and testing data with features and a target.\n",
    "\n",
    "    Keyword arguments:\n",
    "    input_name (str) -- what you want to call the model. it will be saved in a csv later.\n",
    "    input_model (sklearn model) -- model.\n",
    "    training_data (pd.DataFrame)\n",
    "    testing_data (pd.DataFrame)\n",
    "    features (list) -- list of the column names used in this model to create the predictions.\n",
    "    features (str) -- name of column that we want to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set training/test X/y\n",
    "    X_train = training_data[features]\n",
    "    y_train = training_data[target]\n",
    "    X_test = testing_data[features]\n",
    "    y_test = testing_data[target]\n",
    "    \n",
    "    # fit the model that was passed in\n",
    "    # model = input_model()\n",
    "    model = clone(input_model) #testing\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # get predictions on the test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # generate stats (dataframe) of the accuracy, precision, recall, f1, and roc auc\n",
    "    stats = helper_acc_stats(\n",
    "        y_actual_test = y_test, \n",
    "        y_pred_test = y_test_pred,\n",
    "        model_name = input_name, \n",
    "        features = features,\n",
    "    )\n",
    "    # write the stats to a csv so we can look at\n",
    "    helper_write(stats = stats, file_path = 'results.csv')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3c0f9",
   "metadata": {},
   "source": [
    "### 3d) Feature Selection + Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6b9f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(model, data, all_feats, target, model_name):\n",
    "    \n",
    "    # set train/test split\n",
    "    train_df, test_df = set_train_test(data = data, input_seed = 1, pct_test = 0.2)\n",
    "    \n",
    "    best_model_feats = [] # stores best model feats\n",
    "    for i in range(len(all_feats)):\n",
    "        \n",
    "        prop_feats = [] # proposed feats to add\n",
    "        best_model_roc_auc = -999999999999\n",
    "        \n",
    "        for c in all_feats:\n",
    "            # if the feat that we are considering adding is already in our current best model, then skip\n",
    "            if c in best_model_feats: \n",
    "                continue\n",
    "\n",
    "            # this is a list of the features we are going to test for this iteration\n",
    "            feats_to_test = best_model_feats.copy()\n",
    "            feats_to_test.append(c)\n",
    "\n",
    "            fitted_model = evaluate_model(model_name, model, train_df, test_df, feats_to_test, target)\n",
    "            \n",
    "            y_pred = fitted_model.predict(test_df[feats_to_test])\n",
    "            y_actual = test_df[target]\n",
    "\n",
    "            stats = helper_acc_stats(y_actual, y_pred, model_name, feats_to_test)\n",
    "            roc_auc_stat = stats.loc[0,'test_roc_auc']\n",
    "\n",
    "            if roc_auc_stat > best_model_roc_auc:\n",
    "\n",
    "                best_model_roc_auc = roc_auc_stat \n",
    "                prop_feats = feats_to_test\n",
    "                \n",
    "        best_model_feats = prop_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586626cb",
   "metadata": {},
   "source": [
    "## 4. Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35048dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define starting features\n",
    "def starting_features(data):\n",
    "    features = []\n",
    "    for i in data.columns:\n",
    "        if i != 'Loan_Status':\n",
    "            features.append(i)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8007e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_features = starting_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee39061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no CSV called results.csv\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression model\n",
    "forward_selection(LogisticRegression(max_iter = 500), df, starting_features, 'Loan_Status', 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8915be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run random forest regression model\n",
    "'''\n",
    "You need a bit more than 8GB RAM for predicting 2 classes with 256 trees.\n",
    "Of course, you can use a lower number, but gradually you'll notice worse performance.\n",
    "'''\n",
    "forward_selection(RandomForestClassifier(n_estimators=256), df, starting_features, 'Loan_Status', 'Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c4d61",
   "metadata": {},
   "source": [
    "Based on 'results.csv', the Random Forest Classifier performed best with the following features:<br>\n",
    "- 'Credit_History'\n",
    "- 'Gender'\n",
    "- 'Married'\n",
    "- 'Dependents'\n",
    "- 'Education'\n",
    "- 'Loan_Amount_Term_Months'\n",
    "- 'Property_Area'\n",
    "- 'Applicant_Income(total yearly)_to_Loan_Amount(total)'\n",
    "\n",
    "First, let's make sure our training results were similar and that our model is not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1547751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features from top performing model\n",
    "features = ['Credit_History', 'Gender', 'Married', 'Dependents', 'Education', 'Loan_Amount_Term_Months', 'Property_Area', 'Applicant_Income(total yearly)_to_Loan_Amount(total)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a51df5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test/train data\n",
    "test_train_RF_Classifier = set_train_test(data = df, input_seed = 1, pct_test = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72584c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluate_model and save returned model as new variable\n",
    "RF_Class_model = evaluate_model(input_name = 'test_RF_Class', input_model = RandomForestClassifier(n_estimators=256) , training_data=test_train_RF_Classifier[0], testing_data=test_train_RF_Classifier[1], features=features, target='Loan_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4853b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X_train\n",
    "X_train = test_train_RF_Classifier[0][features]\n",
    "\n",
    "# define y predicted for the training data\n",
    "y_pred_train = RF_Class_model.predict(X_train)\n",
    "\n",
    "# define y actual for the training data\n",
    "y_actual_train = test_train_RF_Classifier[0]['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4230bda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate roc auc for train data\n",
    "train_roc_auc = roc_auc_score(y_actual_train, y_pred_train)\n",
    "train_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014bb76",
   "metadata": {},
   "source": [
    "The train_roc_auc = 1 suggests our model is overfit to the training data. As a way to avoid overfit in Random Forest models, we can prune the decision trees. In the hyperparameters this is reducing the 'max_depth'.\n",
    "\n",
    "Additionally, I will reduce the number of variables samples at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afa8e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_selection(RandomForestClassifier(n_estimators=256, max_depth = 5, max_features = .5), df, starting_features, 'Loan_Status', 'Random Forest Classifier Hyperparameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b92b26c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7123632458554265"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select features from top performing model\n",
    "features = ['Credit_History', 'Applicant_Income(total yearly)_to_Loan_Amount(total)', 'Gender']\n",
    "\n",
    "# get test/train data\n",
    "test_train_RF_Classifier_Hyperparameter = set_train_test(data = df, input_seed = 1, pct_test = 0.2)\n",
    "\n",
    "# run evaluate_model and save returned model as new variable\n",
    "RF_Class__Hyperparameter_model = evaluate_model(input_name = 'test_RF_hyper_Class', input_model = RandomForestClassifier(n_estimators=256, max_depth = 5, max_features = .5), training_data=test_train_RF_Classifier_Hyperparameter[0], testing_data=test_train_RF_Classifier_Hyperparameter[1], features=features, target='Loan_Status')\n",
    "\n",
    "# define X_train\n",
    "X_train = test_train_RF_Classifier_Hyperparameter[0][features]\n",
    "\n",
    "# define y predicted for the training data\n",
    "y_pred_train = RF_Class__Hyperparameter_model.predict(X_train)\n",
    "\n",
    "# define y actual for the training data\n",
    "y_actual_train = test_train_RF_Classifier_Hyperparameter[0]['Loan_Status']\n",
    "\n",
    "# calculate roc auc for train data\n",
    "train_roc_auc = roc_auc_score(y_actual_train, y_pred_train)\n",
    "train_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1449d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
